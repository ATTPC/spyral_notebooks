{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Physics Parameters\n",
    "\n",
    "This notebook shows how the estimation phase of Spyral works. To use this notebook the ClusterPhase of Spyral *must* have been run on the data. Once clusters have been identified, the next step is to estimate the physics parameters which will be feed to the solver phase (either InterpSolverPhase or InterpLeastSqSolverPhase). For more details on the different phases, see the Spyral [documentation](https://attpc.github.io/Spyral).\n",
    "\n",
    "## Setup\n",
    "First we import  the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyral.core.cluster import Cluster\n",
    "from spyral.core.estimator import estimate_physics\n",
    "from spyral.geometry.circle import generate_circle_points\n",
    "from spyral.core.run_stacks import form_run_string\n",
    "from spyral import EstimateParameters, DetectorParameters\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our modules, we need to specifiy our configuration as per usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "workspace_path = Path(\"/path/to/your/workspace/\")\n",
    "\n",
    "est_params = EstimateParameters(\n",
    "    min_total_trajectory_points=30, smoothing_factor=100.0\n",
    ")\n",
    "\n",
    "det_params = DetectorParameters(\n",
    "    magnetic_field=2.85,\n",
    "    electric_field=45000.0,\n",
    "    detector_length=1000.0,\n",
    "    beam_region_radius=25.0,\n",
    "    micromegas_time_bucket=10.0,\n",
    "    window_time_bucket=560.0,\n",
    "    get_frequency=6.25,\n",
    "    garfield_file_path=Path(\"/path/to/some/garfield.txt\"),\n",
    "    do_garfield_correction=False,\n",
    ")\n",
    "\n",
    "cluster_path = workspace_path / \"Cluster\" # This may change if you add custom phases!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open our file and make our event iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_number = 16\n",
    "cluster_file_path = cluster_path / f\"{form_run_string(run_number)}.h5\"\n",
    "cluster_file = h5.File(cluster_file_path, \"r\")\n",
    "cluster_group = cluster_file[\"cluster\"]\n",
    "min_event = cluster_group.attrs[\"min_event\"]\n",
    "max_event = cluster_group.attrs[\"max_event\"]\n",
    "event_iter = iter(range(min_event, max_event+1))\n",
    "print(f\"First event: {min_event} Last event: {max_event}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Re-running the code below this cell will walk through the events in order, so long as the cells above are not re-run.\n",
    "\n",
    "First we select a specific event to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "nclusters = 0\n",
    "event = None\n",
    "# You can also hardcode an event if you want!\n",
    "# event = 2\n",
    "if event is None:\n",
    "    try:\n",
    "        event = next(event_iter)\n",
    "    except StopIteration:\n",
    "        raise Exception(\"You ran out of events in this file (wow!), open a new file to look at.\")\n",
    "event_name = f\"event_{event}\"\n",
    "if event_name not in cluster_group:\n",
    "    raise Exception(\"This was a downscale beam event and was removed from the dataset! Rerun this cell to select a new event!\")\n",
    "event_group = cluster_group[f\"event_{event}\"]\n",
    "nclusters = event_group.attrs[\"nclusters\"]\n",
    "if nclusters == 0:\n",
    "    raise Exception(f\"There are no clusters for event {event}, run this cell again!\")\n",
    "cluster_iter = iter(range(0, nclusters))\n",
    "print(f\"Event: {event}\")\n",
    "print(f\"N Clusters: {nclusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we\"ll select a specific cluster to look at from that event (assuming the event has some clusters! If you got an error in the above cell, run it again!). If you run the cell below without running the cell above, you\"ll walk through the set of clusters for a given event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_index = None\n",
    "# You can also hardcode a cluster if you want!\n",
    "# cluster_index = 0\n",
    "if cluster_index is None:\n",
    "    try:\n",
    "        cluster_index = next(cluster_iter)\n",
    "    except StopIteration:\n",
    "        raise Exception(\"You ran out of clusters for this event, move to the next event (run the cell above this one)\")\n",
    "local_cluster = event_group[f\"cluster_{cluster_index}\"]\n",
    "cluster = Cluster(event, local_cluster.attrs[\"label\"], local_cluster[\"cloud\"][:].copy())\n",
    "\n",
    "print(f\"Cluster index: {cluster_index}\")\n",
    "print(f\"Cluster size: {len(cluster.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our cluster selected and loaded, we can now send it, along with some configuration paramters, through the estimator code and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = estimate_physics(\n",
    "    cluster_index, \n",
    "    cluster, \n",
    "    event_group.attrs[\"ic_amplitude\"], \n",
    "    event_group.attrs[\"ic_centroid\"], \n",
    "    event_group.attrs[\"ic_integral\"], \n",
    "    event_group.attrs[\"ic_multiplicity\"], \n",
    "    event_group.attrs[\"orig_run\"],\n",
    "    event_group.attrs[\"orig_event\"],\n",
    "    est_params, \n",
    "    det_params, \n",
    ")\n",
    "if result is None:\n",
    "    raise Exception(\"This cluster failed the estimation analysis, try a different one!\")\n",
    "\n",
    "# The plotting\n",
    "\n",
    "rho_mm = result.brho/det_params.magnetic_field * 1000.0 * np.sin(result.polar)\n",
    "length_samples = np.linspace(1.0, 50.0, 50)\n",
    "dir_x_samples = length_samples * np.cos(result.azimuthal) + result.vertex_x\n",
    "dir_y_samples = length_samples * np.sin(result.azimuthal) + result.vertex_y\n",
    "circle_points = generate_circle_points(result.center_x, result.center_y, rho_mm)\n",
    "beam_region = generate_circle_points(0., 0., det_params.beam_region_radius)\n",
    "print(f\"Brho(T*m): {result.brho}\")\n",
    "print(f\"Rho(mm): {rho_mm}\")\n",
    "print(f\"dEdx: {result.dEdx}\")\n",
    "print(f\"Polar(deg):{result.polar * 180.0/np.pi}\")\n",
    "print(f\"Azimuthal(deg):{result.azimuthal * 180.0/np.pi}\")\n",
    "print(f\"Direction: {result.direction}\")\n",
    "print(f\"Circle center x: {result.center_x} y: {result.center_y}\")\n",
    "print(f\"Vertex z: {result.vertex_z} x: {result.vertex_x} y: {result.vertex_y} Cluster z start: {cluster.data[0, 2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplot_mosaic(\n",
    "    \"\"\"\n",
    "    AAB\n",
    "    \"\"\",\n",
    "    per_subplot_kw={\n",
    "        \"A\": {\n",
    "            \"projection\": \"3d\", \n",
    "            \"box_aspect\": (2,1,1),\n",
    "            \"aspect\": \"equalxy\"\n",
    "        }\n",
    "    },\n",
    "    figsize=(15.0, 5.0),\n",
    "    constrained_layout=True\n",
    ")\n",
    "axs[\"A\"].scatter(cluster.data[:, 2], cluster.data[:, 0], cluster.data[:, 1], c=cluster.data[:, 3], s=3, label=\"Cluster\")\n",
    "axs[\"A\"].scatter(cluster.data[0, 2], cluster.data[0, 0], cluster.data[0, 1], c=\"red\", label=\"Cluster Start\")\n",
    "axs[\"A\"].scatter([result.vertex_z], [result.vertex_x], [result.vertex_y], c=\"lime\", label=\"Vertex\")\n",
    "axs[\"A\"].set_xlim3d(0., 1000.0)\n",
    "axs[\"A\"].set_xlabel(\"Z(mm)\")\n",
    "axs[\"A\"].set_ylim3d(-300.0, 300.0)\n",
    "axs[\"A\"].set_ylabel(\"X(mm)\")\n",
    "axs[\"A\"].set_zlim3d(-300.0, 300.0)\n",
    "axs[\"A\"].set_zlabel(\"Y(mm)\")\n",
    "axs[\"B\"].scatter(cluster.data[:, 0], cluster.data[:, 1], c=cluster.data[:, 3], s=3, label=\"Cluster\")\n",
    "axs[\"B\"].scatter(cluster.data[0, 0], cluster.data[0, 1], c=\"red\", label=\"Cluster Start\")\n",
    "axs[\"B\"].scatter([result.vertex_x], [result.vertex_y], c=\"lime\", label=\"Vertex\")\n",
    "axs[\"B\"].plot(beam_region[:, 0], beam_region[:, 1], label=\"Beam Region\")\n",
    "axs[\"B\"].plot(circle_points[:, 0], circle_points[:, 1], label=\"Circle Fit\")\n",
    "axs[\"B\"].plot(dir_x_samples, dir_y_samples, label=\"Initial Direction\")\n",
    "axs[\"B\"].set_xlim(-300.0, 300.0)\n",
    "axs[\"B\"].set_xlabel(\"X(mm)\")\n",
    "axs[\"B\"].set_ylim(-300.0, 300.0)\n",
    "axs[\"B\"].set_ylabel(\"Y(mm)\")\n",
    "axs[\"B\"].grid()\n",
    "axs[\"B\"].legend()\n",
    "axs[\"A\"].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at some of our results! First we'll look at the circle fit, and the estimated vertex poistion, which is how we estimate $B\\rho$. We'll also draw a line corresponding to the estimated initial direction. One important part of the estimation phase is smoothing. Smoothing splines are applied to the x, y, and charge coordinates as a function of z. So your cluster might look a little different than before, but that is by design. The smoothing helps us get accurate measures for important quantities like dEdx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way you can examine and tune the parameters for the estimation phase. If you want to make complete particle ID plots, it is recommended to use the particle_id.ipynb rather than these notebooks, which are more for demonstration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
