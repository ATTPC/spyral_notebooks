{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining Physics Using the Interpolation Method\n",
    "\n",
    "This notebook demonstrates the final phase of Spyral, solving. To run this notebook, the EstimatePhase of Spyral *must* have been run on the data. Now that we have generated our clusters and estimated the physical observables of interest we are ready to initiate the solving phase of the analysis, where we attempt to extract the exact physics observables by fitting solutions of the equations of motion to the data. This works by pre-generating a bunch of solutions to the ODE's and then interpolation on these solutions to try and fit the data. It has the advantage of being very fast; the ODE's only ever need to be solved once, and then all the remaining calculation is just simple bilinear interpolation. For more information on the method, see the Spyral [documentation](https://attpc.github.io/Spyral)\n",
    "\n",
    "## Setup\n",
    "First let's take care of all of our imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyral.core.cluster import Cluster\n",
    "from spyral.interpolate.track_interpolator import create_interpolator\n",
    "\n",
    "# Pick one of these import lines to uncomment to use as your solver\n",
    "# By default we chose the L-BFGS-B\n",
    "from spyral.solvers.solver_interp import fit_model_interp, Guess, interpolate_trajectory\n",
    "# from spyral.solvers.solver_interp_leastsq import fit_model_interp, Guess, interpolate_trajectory\n",
    "from spyral.phases.interp_solver_phase import DEFAULT_PID_XAXIS, DEFAULT_PID_YAXIS\n",
    "\n",
    "from spyral.core.run_stacks import form_run_string\n",
    "from spyral import SolverParameters, DetectorParameters, InterpSolverPhase\n",
    "\n",
    "from spyral_utils.nuclear import NuclearDataMap\n",
    "from spyral_utils.nuclear.particle_id import deserialize_particle_id\n",
    "\n",
    "import polars as pl\n",
    "import h5py as h5\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with all of our code imported we will setup the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "workspace_path = Path(\"/path/to/your/workspace/\")\n",
    "\n",
    "solver_params = SolverParameters(\n",
    "    gas_data_path=Path(\"/path/to/your/gas.json\"),\n",
    "    particle_id_filename=Path(\"/path/to/your/pid.json\"),\n",
    "    ic_min_val=900.0,\n",
    "    ic_max_val=1300.0,\n",
    "    n_time_steps=1000,\n",
    "    interp_ke_min=0.1,\n",
    "    interp_ke_max=70.0,\n",
    "    interp_ke_bins=700,\n",
    "    interp_polar_min=5.0,\n",
    "    interp_polar_max=85.0,\n",
    "    interp_polar_bins=160,\n",
    "    fit_vertex_rho=True,\n",
    "    fit_vertex_phi=True,\n",
    "    fit_azimuthal=True,\n",
    "    fit_method=\"lbfsgsb\" # has no impact here, we do this ourselves at the imports\n",
    ")\n",
    "\n",
    "det_params = DetectorParameters(\n",
    "    magnetic_field=2.85,\n",
    "    electric_field=45000.0,\n",
    "    detector_length=1000.0,\n",
    "    beam_region_radius=25.0,\n",
    "    micromegas_time_bucket=10.0,\n",
    "    window_time_bucket=560.0,\n",
    "    get_frequency=6.25,\n",
    "    garfield_file_path=Path(\"/path/to/some/garfield.txt\"),\n",
    "    do_garfield_correction=False,\n",
    ")\n",
    "\n",
    "cluster_path = workspace_path / \"Cluster\" # this may change if you add custom phases\n",
    "estimate_path = workspace_path / \"Estimation\" # this may change if you add custom phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to load our interpolation mesh. Note that if you don't have one created, the below cell will create it for you and this can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuc_map = NuclearDataMap()\n",
    "pid = deserialize_particle_id(solver_params.particle_id_filename, nuc_map)\n",
    "if pid is None:\n",
    "    raise Exception(\"Particle ID error!\")\n",
    "pid_xaxis = DEFAULT_PID_XAXIS\n",
    "pid_yaxis = DEFAULT_PID_YAXIS\n",
    "if not pid.cut.is_default_x_axis() and not pid.cut.is_default_y_axis():\n",
    "    pid_xaxis = pid.cut.get_x_axis()\n",
    "    pid_yaxis = pid.cut.get_y_axis()\n",
    "solver = InterpSolverPhase(solver_params, det_params)\n",
    "success = solver.create_assets(workspace_path)\n",
    "if not success:\n",
    "    raise Exception(\"Could not setup interpolation mesh!\")\n",
    "tracks = create_interpolator(solver.track_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll load a specific run file (both the estimation result and clustering result) and make an iterator to run through the events in the data. Here we also apply the particle ID gate to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_number = 16\n",
    "cluster_file_path = cluster_path / f\"{form_run_string(run_number)}.h5\"\n",
    "cluster_file = h5.File(cluster_file_path, \"r\")\n",
    "estimate_file_path = estimate_path / f\"{form_run_string(run_number)}.parquet\"\n",
    "estimate_df = pl.scan_parquet(estimate_file_path)\n",
    "estimate_gated = estimate_df.filter(pl.struct([pid_xaxis, pid_yaxis]).map_batches(pid.cut.is_cols_inside)).collect().to_dict()\n",
    "cluster_group = cluster_file[\"cluster\"]\n",
    "nrows = len(estimate_gated[\"event\"])\n",
    "row_iter = iter(range(nrows))\n",
    "print(f\"Number of rows: {nrows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Re-running the cells below will walk through the events in the dataset in order, as long as you don't re-run the cells above.\n",
    "\n",
    "Now we'll load the next event from the dataset. You can always use a hardcoded value to select a specific event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "row = None\n",
    "# Can always override with a hardcoded row if needed\n",
    "# row = 1\n",
    "if row is None:\n",
    "    try:\n",
    "        row = next(row_iter)\n",
    "    except StopIteration:\n",
    "        raise Exception(\"You ran out of rows for this run! Open a new run\")\n",
    "    \n",
    "\n",
    "print(f\"Row: {row}\")\n",
    "event = estimate_gated['event'][row]\n",
    "cluster_index = estimate_gated['cluster_index'][row]\n",
    "print(f\"Event: {event}\")\n",
    "print(f\"Cluster index: {cluster_index}\")\n",
    "event_group = cluster_group[f\"event_{event}\"]\n",
    "local_cluster = event_group[f\"cluster_{cluster_index}\"]\n",
    "print(f'Direction: {estimate_gated[\"direction\"][row]}')\n",
    "cluster = Cluster(event, local_cluster.attrs[\"label\"], local_cluster[\"cloud\"][:].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our cluster and estimated observables loaded, we are ready to fit to the data. We setup our Guess object from our estimates and then pass that along to the fit_model function. Sometimes this will return None when a given trajectory has estimates that are outside the interpolation table (these typically correspond to bad events). If this happens a error will occur. Simply re-run the notebook until the a good event is randomly selected. Note that the first time you run this block it might take a couple of seconds. This is because the interpolation method uses a just-in-time compiler (jit) to speed up the calculations. The first time you call the code, the code gets compiled (resulting in a slowdown). But everytime the code is called after that, the compiled program is used, resulting in enormus performance gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = Guess(\n",
    "    estimate_gated[\"polar\"][row],\n",
    "    estimate_gated[\"azimuthal\"][row],\n",
    "    estimate_gated[\"brho\"][row],\n",
    "    estimate_gated[\"vertex_x\"][row],\n",
    "    estimate_gated[\"vertex_y\"][row],\n",
    "    estimate_gated[\"vertex_z\"][row]\n",
    ")\n",
    "print(guess)\n",
    "result = fit_model_interp(cluster, guess, pid.nucleus, tracks, det_params, solver_params)\n",
    "if result is None:\n",
    "    print(\"Guess outside of interpolation range!\")\n",
    "best_fit_trajectory = interpolate_trajectory(result, tracks, pid.nucleus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a good event was chosen, you should see above a print out of the fit results. Key values are the chi-square (which should be small) and the variable values, which are the fitted observables. Also important are the correlations, which tell you if any of the parameters are co-dependent. If two parameters have a correlation of 1.0 they are basically degenerate to the fitter, which is very bad.\n",
    "\n",
    "We can also plot the results of the fit against the data to vizualize the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplot_mosaic(\n",
    "    \"\"\"\n",
    "    AB\n",
    "    AC\n",
    "    \"\"\",\n",
    "    constrained_layout=True,\n",
    "    figsize=(16,8)\n",
    ")\n",
    "\n",
    "axs[\"A\"].scatter(cluster.data[:, 0]*0.001, cluster.data[:, 1]*0.001, s=3, label=\"Cluster\")\n",
    "axs[\"A\"].scatter(best_fit_trajectory[:, 0], best_fit_trajectory[:, 1], s=3, label=\"Best Fit\")\n",
    "axs[\"A\"].scatter([result[\"vertex_x\"]], [result[\"vertex_y\"]], s=3, label=\"Best Fit Vertex\")\n",
    "axs[\"A\"].set_xlabel(\"X (m)\")\n",
    "axs[\"A\"].set_ylabel(\"Y (m)\")\n",
    "axs[\"A\"].set_xlim(-0.3, 0.3)\n",
    "axs[\"A\"].set_ylim(-0.3, 0.3)\n",
    "axs[\"B\"].scatter(cluster.data[:, 2]*0.001, cluster.data[:, 1]*0.001, s=3, label=\"Cluster\")\n",
    "axs[\"B\"].scatter(best_fit_trajectory[:, 2], best_fit_trajectory[:, 1], s=3, label=\"Best Fit\")\n",
    "axs[\"B\"].scatter([result[\"vertex_z\"]], [result[\"vertex_y\"]], s=3, label=\"Best Fit Vertex\")\n",
    "axs[\"B\"].set_xlabel(\"Z (m)\")\n",
    "axs[\"B\"].set_ylabel(\"Y (m)\")\n",
    "axs[\"B\"].set_xlim(.0, 1.0)\n",
    "axs[\"B\"].set_ylim(-0.3, 0.3)\n",
    "axs[\"C\"].scatter(cluster.data[:, 2]*0.001, cluster.data[:, 0]*0.001, s=3, label=\"Cluster\")\n",
    "axs[\"C\"].scatter(best_fit_trajectory[:, 2], best_fit_trajectory[:, 0], s=3, label=\"Best Fit\")\n",
    "axs[\"C\"].scatter([result[\"vertex_z\"]], [result[\"vertex_x\"]], s=3, label=\"Best Fit Vertex\")\n",
    "axs[\"C\"].set_xlabel(\"Z (m)\")\n",
    "axs[\"C\"].set_ylabel(\"X (m)\")\n",
    "axs[\"C\"].set_xlim(.0, 1.0)\n",
    "axs[\"C\"].set_ylim(-0.3, 0.3)\n",
    "axs[\"A\"].legend()\n",
    "axs[\"B\"].legend()\n",
    "axs[\"C\"].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you see a nice fit to the data! If the fit looks bad, there are several things to check. First is the particle ID gate; if the wrong particle group is selected, the fit will fail spectacularly. Another is the coarse-ness of the interpolation scheme. If there are too few bins in the polar angle or the particle kinetic energy, the interpolation may not generate good values. Finally, it is also good to make sure that the target gas is correctly defined with the right pressure and chemistry.\n",
    "\n",
    "To walk through more events, you can re-run the cells under the Analysis heading, and you will walk through the events in the order they were written."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
